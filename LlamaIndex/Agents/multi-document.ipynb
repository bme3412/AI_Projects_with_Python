{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980adb95-c888-4b2d-a4ac-d5f230250975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "\n",
    "from pathlib import Path\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363a7fca-f987-493f-9121-7ed697ee3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_paths = [\n",
    "    Path(\"goog-10-k-2023.pdf\"),\n",
    "    Path(\"goog-10-k-2022.pdf\"),\n",
    "    # Add more file paths...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fdf280c-8dc6-413c-ac17-27807bfe161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in pdf_file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        pdf_text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_text += page.extract_text()\n",
    "\n",
    "    data_path = Path(\"data\")\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{file_path.stem}.txt\", \"w\") as fp:\n",
    "        fp.write(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd370e6-00ad-4091-adea-dcc7d6beb66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 10-K PDF documents\n",
    "pdf_docs = {}\n",
    "for file_path in pdf_file_paths:\n",
    "    pdf_docs[file_path.stem] = SimpleDirectoryReader(\n",
    "        input_files=[f\"data/{file_path.stem}.txt\"]\n",
    "    ).load_data()\n",
    "\n",
    "# Define Global LLM and Embeddings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99928b78-bb30-4b37-b063-79e0606213c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Document Agent for each Document\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "\n",
    "node_parser = SentenceSplitter()\n",
    "\n",
    "# Build agents dictionary\n",
    "agents = {}\n",
    "query_engines = {}\n",
    "\n",
    "# this is for the baseline\n",
    "all_nodes = []\n",
    "\n",
    "for idx, file_path in enumerate(pdf_file_paths):\n",
    "    nodes = node_parser.get_nodes_from_documents(pdf_docs[file_path.stem])\n",
    "    all_nodes.extend(nodes)\n",
    "\n",
    "    if not os.path.exists(f\"./data/{file_path.stem}\"):\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes)\n",
    "        vector_index.storage_context.persist(\n",
    "            persist_dir=f\"./data/{file_path.stem}\"\n",
    "        )\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=f\"./data/{file_path.stem}\"),\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)\n",
    "    summary_query_engine = summary_index.as_query_engine(llm=Settings.llm)\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"vector_tool\",\n",
    "                description=(\n",
    "                    \"Useful for questions related to specific aspects of\"\n",
    "                    f\" {file_path.stem} (e.g. financial statements, risk factors, management discussion and analysis, or more).\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"summary_tool\",\n",
    "                description=(\n",
    "                    \"Useful for any requests that require a holistic summary\"\n",
    "                    f\" of EVERYTHING about {file_path.stem}. For questions about\"\n",
    "                    \" more specific sections, please use the vector_tool.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-4\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about {file_path.stem}.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    agents[file_path.stem] = agent\n",
    "    query_engines[file_path.stem] = vector_index.as_query_engine(\n",
    "        similarity_top_k=2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb6dd34-d0e3-4f8e-8e71-e67dacc52c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Retriever-Enabled OpenAI Agent\n",
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for file_path in pdf_file_paths:\n",
    "    pdf_summary = (\n",
    "        f\"This content contains information from the 10-K PDF: {file_path.stem}. Use\"\n",
    "        f\" this tool if you want to answer any questions about {file_path.stem}.\\n\"\n",
    "    )\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agents[file_path.stem],\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{file_path.stem}\",\n",
    "            description=pdf_summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71520dd5-2745-4748-85bb-37ff03c3c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What were the total revenues for the company in the latest fiscal year? What were the total revenues for the company in  2022?\n",
      "=== Calling Function ===\n",
      "Calling function: tool_goog-10-k-2023 with args: {\"input\": \"total revenues\"}\n",
      "Added user message to memory: total revenues\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"total revenues\"\n",
      "}\n",
      "Got output: Total revenues for the year ended December 31, 2023 were $307.4 billion.\n",
      "========================\n",
      "\n",
      "Got output: The total revenues for the year ended December 31, 2023 were $307.4 billion.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: tool_goog-10-k-2022 with args: {\"input\": \"total revenues\"}\n",
      "Added user message to memory: total revenues\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"total revenues\"\n",
      "}\n",
      "Got output: Total revenues for the company are generated through various sources such as brand advertising, Google Cloud services, Google other revenues (including Google Play, hardware sales, YouTube non-advertising), and subscription-based products. The company also recognizes revenues from multiple performance obligations in contracts with customers and factors in customer incentives and credits. Additionally, revenues are impacted by factors like changes in foreign currency exchange rates, pricing adjustments, economic conditions, new product launches, and seasonality.\n",
      "========================\n",
      "\n",
      "Got output: The total revenues for the company are generated from a variety of sources. These include brand advertising, Google Cloud services, Google other revenues (which encompasses Google Play, hardware sales, and YouTube non-advertising), and subscription-based products. The company also recognizes revenues from multiple performance obligations in contracts with customers, taking into account customer incentives and credits. The total revenues are also influenced by factors such as changes in foreign currency exchange rates, pricing adjustments, economic conditions, the launch of new products, and seasonality. However, the exact figures for the total revenues are not provided in the response.\n",
      "========================\n",
      "\n",
      "The total revenues for the company in the latest fiscal year, which ended on December 31, 2023, were $307.4 billion. \n",
      "\n",
      "For the fiscal year 2022, the exact total revenues figure is not provided in the response.\n"
     ]
    }
   ],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")\n",
    "\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "top_agent = OpenAIAgent.from_tools(\n",
    "    tool_retriever=obj_index.as_retriever(similarity_top_k=3),\n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries about a set of given 10-K PDF documents.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Define Baseline Vector Store Index\n",
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)\n",
    "\n",
    "# Running Example Queries\n",
    "# Replace with your own example queries\n",
    "response = top_agent.query(\"What were the total revenues for the company in the latest fiscal year? What were the total revenues for the company in  2022?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79584c-87b6-46fb-858a-26ae44d10848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
