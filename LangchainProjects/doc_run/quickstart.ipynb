{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17365fac-049a-4254-97ee-73378cc0122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e3180e-81fa-4bcf-a682-f1c5734889a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in several ways:\\n\\n1. Automated Testing: Langsmith provides a framework for creating and running automated tests. It supports various types of tests such as unit tests, integration tests, and end-to-end tests. Automated testing helps in quickly identifying issues and regressions in the codebase.\\n\\n2. Test Data Generation: Langsmith can generate test data to simulate various scenarios and edge cases. This is helpful in validating the behavior of the software in different scenarios and ensuring its robustness.\\n\\n3. Test Coverage Analysis: Langsmith can analyze the codebase and provide insights into the test coverage. It can identify areas of the code that are not adequately covered by tests, helping developers and testers prioritize their efforts in writing additional tests.\\n\\n4. Mocking and Stubbing: Langsmith provides tools for creating mocks and stubs, which are useful in isolating components for testing. By creating mock objects, dependencies can be controlled and manipulated to simulate specific behaviors or responses, enabling thorough testing of individual components.\\n\\n5. Performance Testing: Langsmith can be used to simulate high-load scenarios and measure the performance of the software under varying conditions. It helps in identifying potential bottlenecks and performance issues early on.\\n\\n6. Continuous Integration and Deployment: Langsmith integrates with CI/CD pipelines, allowing tests to be automatically executed whenever changes are made to the software. This ensures that any introduced bugs or issues are identified and resolved quickly, improving the overall software quality.\\n\\nOverall, Langsmith provides a comprehensive set of tools and features that can assist in various aspects of testing, making the testing process more efficient and effective.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('how can langsmith help with testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7beb511f-c16f-43a6-9a07-ba950501c9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in several ways:\\n\\n1. Test Case Management: Langsmith provides a centralized platform to manage test cases, allowing testers to create, update, and organize test cases in a structured manner. Test cases can be categorized, prioritized, and linked to requirements, making it easier to track the progress of testing activities.\\n\\n2. Test Execution: Langsmith allows testers to execute test cases directly from the platform, eliminating the need for manual tracking and documentation. Testers can easily record the test results, attach screenshots or logs, and add comments for each test case execution.\\n\\n3. Test Coverage Analysis: Langsmith provides insights into test coverage, allowing testers to track which areas of the software have been tested and which areas still need to be covered. This helps in identifying any potential gaps in testing and ensures comprehensive coverage.\\n\\n4. Defect Tracking: Langsmith includes a defect tracking system that enables testers to log and track defects found during testing. Testers can provide detailed information about the defect, including steps to reproduce, severity, and priority. The system also allows for collaboration between testers and developers to resolve and retest defects.\\n\\n5. Test Reporting: Langsmith offers customizable reports and dashboards that provide a comprehensive view of testing progress and results. Testers can generate reports on test execution status, defects, test coverage, and more, making it easier to communicate testing outcomes to stakeholders.\\n\\n6. Integration with other Testing Tools: Langsmith can integrate with various testing tools, such as test automation frameworks or test management tools. This integration allows for seamless collaboration between different testing activities and helps streamline the overall testing process.\\n\\nOverall, Langsmith helps testers streamline their testing activities, improve test coverage, track defects more effectively, and provide comprehensive reporting for better decision-making.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are world class technical documentation writer'),\n",
    "    ('user','{input}')\n",
    "])\n",
    "\n",
    "# combine to a simple LLM chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# invoke and ask same question\n",
    "chain.invoke({'input':'how can langsmith help with testing?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "585b1701-8e16-4a8c-b970-63b7fd2308a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a simple output parser to convert to a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "text = chain.invoke({'input':'how can langsmith help with testing?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ab89bad-9a82-419b-a711-90f473996568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li>Langsmith can help with testing in several ways:</li><li>1. Test Case Management: Langsmith can assist in creating and managing test cases, which are step-by-step instructions to validate the functionality of a system or software. It can help document test case details, including test steps, expected results, and actual results.</li><li>2. Test Plan Documentation: Langsmith can aid in documenting test plans, which outline the overall approach, objectives, resources, and schedule for testing. It enables testers to ensure that all necessary aspects are considered and planned for in the testing process.</li><li>3. User Acceptance Testing (UAT): Langsmith can assist in documenting UAT scenarios and test cases. UAT ensures that the software meets the requirements and expectations of end-users. By documenting UAT scenarios, Langsmith can help testers validate the software's usability and functionality from an end-user perspective.</li><li>4. Bug Reporting: Langsmith can help with documenting and reporting bugs or issues encountered during testing. It allows testers to provide detailed information about the bug, including steps to reproduce, screenshots, and system configurations. Clear and concise bug reports help developers understand and fix the issues efficiently.</li><li>5. Test Automation: Langsmith can assist in documenting test automation scripts and frameworks. Automation helps in executing repetitive and time-consuming tests, reducing manual effort. With Langsmith's documentation, testers can create comprehensive and reusable test scripts, improving the efficiency and effectiveness of test automation.</li><li>6. Performance Testing: Langsmith can help document performance testing procedures, including load testing, stress testing, and scalability testing. It enables testers to measure and evaluate the system's performance under various scenarios, ensuring it meets the required performance benchmarks.</li><li>Overall, Langsmith's documentation capabilities can streamline the testing process, enhance communication between testers and developers, and ensure comprehensive and consistent testing practices.</li></ol>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def text_to_html_list(text):\n",
    "    # Split the text into items based on \"\\n\\n\"\n",
    "    items = text.split(\"\\n\\n\")\n",
    "\n",
    "    # Create an HTML ordered list (numbered list)\n",
    "    html_list = \"<ul>\"\n",
    "\n",
    "    # Add each item as a list element\n",
    "    for item in items:\n",
    "        html_list += f\"<li>{item}</li>\"\n",
    "\n",
    "    # Close the list\n",
    "    html_list += \"</ol>\"\n",
    "\n",
    "    return html_list\n",
    "\n",
    "# Your provided text\n",
    "text = text\n",
    "# Convert to HTML and display\n",
    "HTML(text_to_html_list(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d24590-89dc-472a-bff3-fc302dec8151",
   "metadata": {},
   "source": [
    "### Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb07d420-639d-4fe2-a175-6bd9b6931fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader= WebBaseLoader('https://docs.smith.langchain.com/overview')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a887f017-ba29-4bdc-93ed-2772672d930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc4df435-3b16-465c-b9f4-c89fd02abace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest documents into a vectorstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee43109e-e738-4724-a190-2370768e4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a retrieval chain\n",
    "    ## takes an incoming question\n",
    "    ## looks up relevant documents\n",
    "    ## passes docuemtns into an LLM to ask the question\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fdc2c14-9fbb-4648-b467-e791fe9c55b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing by visualizing test results.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if wanted to, can run ths by passing in documents direct\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    'input':'How can langsmith help with testing?',\n",
    "    'context': [Document(page_content='langsmith can help visualize test results')]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce4b1cd0-6cd6-4a1b-be9c-09a731ce9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but want the documents to come from the retriever that set up\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89b46427-4360-416d-a322-e368aefea58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing the following features:\n",
      "\n",
      "1. Dataset Creation: LangSmith allows users to curate datasets easily, which can be used for testing changes to prompts or chains.\n",
      "\n",
      "2. Running Chains: Users can run chains over the data points in a dataset and visualize the outputs. This helps in manually reviewing the results and identifying any issues or improvements.\n",
      "\n",
      "3. Feedback and Evaluation: LangSmith enables users to assign feedback programmatically to runs. This feedback can be used to track performance over time and pinpoint underperforming data points. LangSmith also provides evaluators to assess the results of test runs and guide users to examples that require manual inspection.\n",
      "\n",
      "4. Human Evaluation: LangSmith offers annotation queues to facilitate human review and annotation of runs. Reviewers can assess subjective qualities like creativity or humor and validate the runs that were auto-evaluated by the system.\n",
      "\n",
      "Overall, LangSmith provides tools and functionalities to support testing and evaluation of models, ensuring quality and reliability in LLM applications.\n"
     ]
    }
   ],
   "source": [
    "# now invoke the chain\n",
    "response = retrieval_chain.invoke({'input':'how can langsmith help with testing?'})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f86ac-7efc-4dd0-bed2-cdda940f868c",
   "metadata": {},
   "source": [
    "### Conversational Retrieval Chain\n",
    "+ enable the chain to handle follow-up questions\n",
    "+ retrieval method needs to take the whole history of context into account\n",
    "+ the final LLM chain should also include the whole history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "410e12c7-e326-4bf1-831b-222ad715051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "## Need a prompt to pass into LLM and query\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user','{input}'),\n",
    "    ('user','Given the above conversation, generate a search query to look up in order to get information relevant to the conversation')\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a18451-4344-4756-b23e-8ba5dc404d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default‚ÄãAt LangChain, all of us have LangSmith‚Äôs tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don‚Äôt even look at the traces, but the 10% of the time that we do‚Ä¶ it‚Äôs so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging‚ÄãDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?‚ÄãLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string ‚Üí string (or chat messages ‚Üí chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?‚ÄãSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?‚ÄãIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring‚ÄãAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We‚Äôve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing ‚Äî mirroring the debug mode approach.We‚Äôve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets‚ÄãLangSmith makes it easy to curate datasets. However, these aren‚Äôt just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.‚Ü©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we‚Äôve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we‚Äôre being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation‚ÄãAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test by passing in instance\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    'input': 'Tell me How'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0e27e65-6f24-43af-820d-e6c95643e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can create a new chain to continue conversation\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "417fb0a2-1d4d-4599-a1e9-b342c7494371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default‚ÄãAt LangChain, all of us have LangSmith‚Äôs tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don‚Äôt even look at the traces, but the 10% of the time that we do‚Ä¶ it‚Äôs so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging‚ÄãDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?‚ÄãLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string ‚Üí string (or chat messages ‚Üí chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?‚ÄãSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?‚ÄãIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring‚ÄãAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We‚Äôve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing ‚Äî mirroring the debug mode approach.We‚Äôve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets‚ÄãLangSmith makes it easy to curate datasets. However, these aren‚Äôt just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.‚Ü©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we‚Äôve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we‚Äôre being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation‚ÄãAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith can help test your LLM applications by providing several features and capabilities. Here are some ways LangSmith can assist in testing:\\n\\n1. Dataset Testing: LangSmith makes it easy to upload datasets and use them to test changes to prompts or chains. You can run the chain over the data points and visualize the outputs. The results can be logged to a new project associated with the dataset for further review.\\n\\n2. Feedback and Evaluation: LangSmith allows you to assign feedback programmatically to runs. If your application has a thumbs up/down button, you can use it to log feedback back to LangSmith. This feedback can be used to track performance over time and pinpoint underperforming data points.\\n\\n3. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified during a test run and will evaluate the results once the test run is complete. While not perfect, they can guide your eye to examples that require closer examination.\\n\\n4. Human Evaluation: LangSmith offers annotation queues that allow you to select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view the input, output, and existing tags, and add your own feedback. This helps in assessing subjective qualities and validating auto-evaluated runs.\\n\\n5. Monitoring: LangSmith can be used to monitor your application in production. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. This helps ensure the reliability and quality of your LLM applications.\\n\\nBy leveraging these testing capabilities of LangSmith, you can evaluate and improve the performance, accuracy, and user experience of your LLM applications.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test end-to-end\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf6265-a00e-4079-bcf0-71445589939a",
   "metadata": {},
   "source": [
    "## Agent\n",
    "+ an agent is where the LLM decides what steps to take\n",
    "\n",
    "+ to start, need to decided which tools it will have acces to\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d537bb9-34f3-4fd3-a210-ab8f649afd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    'langsmith_search',\n",
    "    'Search for information aobut LangSmith. For any questions about LangSmith, you must use this tool!'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7be461-c005-4511-97a9-7cd580d30790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
